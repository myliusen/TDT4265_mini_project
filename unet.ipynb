{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15cff946",
   "metadata": {},
   "source": [
    "# Necessary imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d91965",
   "metadata": {},
   "source": [
    "I used https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d.ipynb as an inspiration, in addition to \n",
    "* https://www.youtube.com/watch?v=83FLt4fPNGs, and\n",
    "* https://www.youtube.com/watch?v=hqgZuatm8eE&t=1946s\n",
    "\n",
    "for the loading and preprocessing parts.\n",
    "\n",
    "Simply change resolution in the preprocessing and comment in the Swin-UNETR model definition to obtain the Swin-UNETR results from the presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24097790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import UNet\n",
    "from monai.data import Dataset, DataLoader, decollate_batch\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, Spacingd, CropForegroundd,\n",
    "    Resized, DivisiblePadd, ToTensord, RandFlipd, RandAffined, RandScaleIntensityd,\n",
    "    RandShiftIntensityd, ScaleIntensityRangePercentilesd, EnsureType, AsDiscrete\n",
    ")\n",
    "from monai.utils import set_determinism, first\n",
    "from monai.networks.nets import UNet\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "# import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.networks.nets.swin_unetr import filter_swinunetr\n",
    "from monai.networks.utils import copy_model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd83ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file paths for train images and labels\n",
    "data_dir = '/datasets/tdt4265/mic/open/HNTS-MRG'\n",
    "train_images = sorted([f for f in glob(os.path.join(data_dir, 'train', '*', 'preRT', '*.nii.gz')) if '_mask' not in f])\n",
    "train_labels = sorted([f for f in glob(os.path.join(data_dir, 'train', '*', 'preRT', '*.nii.gz')) if '_T2' not in f])\n",
    "\n",
    "val_images = sorted([f for f in glob(os.path.join(data_dir, 'test', '*', 'preRT', '*.nii.gz')) if '_mask' not in f])\n",
    "val_labels = sorted([f for f in glob(os.path.join(data_dir, 'test', '*', 'preRT', '*.nii.gz')) if '_T2' not in f])\n",
    "\n",
    "train_files = [{'image': image_name, 'label': label_name} for image_name, label_name in zip(train_images, train_labels)] \n",
    "val_files = [{'image': image_name, 'label': label_name} for image_name, label_name in zip(val_images, val_labels)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f9e5d",
   "metadata": {},
   "source": [
    "Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c9a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load the images\n",
    "# 2) Do necessary transforms (d means for dictionary)\n",
    "# 3) Convert into Monai metatensor\n",
    "\n",
    "original_train_transforms = Compose( # Just to see difference\n",
    "    [\n",
    "        LoadImaged(keys=['image', 'label']),\n",
    "        # Transforms\n",
    "        EnsureChannelFirstd(keys=['image', 'label']),\n",
    "        ToTensord(keys=['image', 'label'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "train_transforms = Compose([\n",
    "    LoadImaged(keys=['image', 'label']),\n",
    "    EnsureChannelFirstd(keys=['image', 'label']),\n",
    "    Spacingd(keys=['image', 'label'], pixdim=(1.5,1.5,2)),\n",
    "    # CropForegroundd(keys=['image', 'label'], source_key='image'),\n",
    "    ScaleIntensityRangePercentilesd(keys='image', lower=0.5, upper=99.5,\n",
    "                               b_min=0.0, b_max=1.0, clip=True),\n",
    "\n",
    "    # --- AUGMENTATION BLOCK ---\n",
    "    RandFlipd(keys=['image','label'], spatial_axis=0, prob=0.10),\n",
    "    RandFlipd(keys=['image','label'], spatial_axis=1, prob=0.10),\n",
    "    RandAffined(\n",
    "        keys=['image','label'],\n",
    "        rotate_range=(0.1,0.1,0.05),    # small rotations (~±6° in-plane)\n",
    "        scale_range=(0.1,0.1,0.05),     # small scaling ±10%\n",
    "        mode=('bilinear','nearest'),\n",
    "        prob=0.15\n",
    "    ),\n",
    "    RandScaleIntensityd(keys='image', factors=0.1, prob=0.10),\n",
    "    RandShiftIntensityd(keys='image', offsets=0.1, prob=0.10),\n",
    "    # -------------------------------\n",
    "\n",
    "    Resized(keys=['image','label'], spatial_size=[256,256,96]),\n",
    "    DivisiblePadd(keys=['image','label'], k=16, mode='constant', constant_values=0),\n",
    "    ToTensord(keys=['image','label'])\n",
    "])\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=['image', 'label']),\n",
    "        # Transforms\n",
    "        EnsureChannelFirstd(keys=['image', 'label']),\n",
    "        Spacingd(keys=['image', 'label'], pixdim=(1.5, 1.5, 2)), # Ensure all voxels are same size\n",
    "        # CropForegroundd(keys=['image', 'label'], source_key='image'),\n",
    "        ScaleIntensityRangePercentilesd(keys='image', lower=0.5, upper=99.5,\n",
    "                               b_min=0.0, b_max=1.0, clip=True),\n",
    "        Resized(keys=['image', 'label'], spatial_size=[256, 256, 96]),\n",
    "        DivisiblePadd(keys=['image', 'label'], k=16, mode='constant', constant_values=0), # pad both image and label\n",
    "        ToTensord(keys=['image', 'label'])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bafe457",
   "metadata": {},
   "source": [
    "Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3620bf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = Dataset(data=train_files, transform=original_train_transforms)\n",
    "original_loader = DataLoader(dataset=original_dataset, batch_size=6) \n",
    "\n",
    "train_dataset = Dataset(data=train_files, transform=train_transforms)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=6)\n",
    "\n",
    "val_dataset = Dataset(data=val_files, transform=val_transforms)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edcb339",
   "metadata": {},
   "source": [
    "Visualize the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59bcb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patient = first(train_loader)\n",
    "original_patient = first(original_loader)\n",
    "\n",
    "plt.figure('test', (12, 6))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.title('Original image of a patient')\n",
    "plt.imshow(original_patient['image'][0, 0, :, :, 30])\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.title('Original label of a patient')\n",
    "plt.imshow(original_patient['label'][0, 0, :, :, 30])\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.title('Transformed image of a patient')\n",
    "plt.imshow(test_patient['image'][0, 0, :, :, 30])\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.title('Label of a patient')\n",
    "plt.imshow(test_patient['label'][0, 0, :, :, 30])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86428e99",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1906f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the 3D UNet model\n",
    "model = UNet(\n",
    "    spatial_dims=3,  # 3D model\n",
    "    in_channels=1,   # Number of input channels (grayscale images)\n",
    "    out_channels=3,  # Number of output classes (foreground & background)\n",
    "    channels=(16, 32, 64, 128, 256),  # Feature map sizes per layer\n",
    "    strides=(2, 2, 2, 2),  # Downsampling factors\n",
    "    num_res_units=2,  # Number of residual units per layer\n",
    "    dropout=0.1 # fight overfitting\n",
    ").to(device)  # Move model to GPU if available\n",
    "\n",
    "\n",
    "# model = SwinUNETR(\n",
    "#     img_size=(160, 160, 96),\n",
    "#     in_channels=1,\n",
    "#     out_channels=3,\n",
    "#     feature_size=48,\n",
    "#     # depths = (2,2,2,2)\n",
    "#     # num_heads=(3,6,12,24)\n",
    "#     dropout_path_rate=0.2\n",
    "# ).to(device)\n",
    "\n",
    "# ckpt = torch.load(\"model_swinvit.pt\", map_location=device, weights_only=True)\n",
    "\n",
    "# model.load_from(ckpt)\n",
    "# print(\"Using pretrained Swin UneTR backbone weights!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67442a4",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5038a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 200\n",
    "val_interval = 5\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "best_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "lr_values = []\n",
    "\n",
    "post_pred = Compose([\n",
    "    EnsureType(),\n",
    "    AsDiscrete(argmax=True, to_onehot=3)\n",
    "])\n",
    "\n",
    "post_label = Compose([\n",
    "    EnsureType(),\n",
    "    AsDiscrete(to_onehot=3)\n",
    "])\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean_batch\", get_not_nans=True)\n",
    "loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5) # add weight decay regularization due to overfitting\n",
    "\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',        \n",
    "    factor=0.5,        \n",
    "    patience=2,        \n",
    "    min_lr=1e-6        \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661ebe53",
   "metadata": {},
   "source": [
    "Training and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb0cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "SEED = 42\n",
    "set_determinism(seed=SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "BEST_MODEL_PATH = f\"{CHECKPOINT_DIR}/best_metric_model.pth\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "def run_validation(model, val_loader, post_pred, post_label, dice_metric, device, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_data in val_loader:\n",
    "            images = val_data[\"image\"].to(device)\n",
    "            labels = val_data[\"label\"].to(device)\n",
    "\n",
    "            outputs = sliding_window_inference(\n",
    "                images, roi_size=(256,256,96), sw_batch_size=2, predictor=model\n",
    "            )\n",
    "\n",
    "            # post‐process into lists of tensors\n",
    "            y_pred = [post_pred(x) for x in decollate_batch(outputs)]\n",
    "            y_true = [post_label(x) for x in decollate_batch(labels)]\n",
    "\n",
    "            dice_metric(y_pred=y_pred, y=y_true)\n",
    "                           \n",
    "    metric, _ = dice_metric.aggregate()\n",
    "    dice_metric.reset()\n",
    "    \n",
    "    dice_bg    = metric[0].item()\n",
    "    dice_cls1  = metric[1].item()\n",
    "    dice_cls2  = metric[2].item()\n",
    "\n",
    "    # average over all three\n",
    "    avg_dice_all = metric.mean().item()\n",
    "    avg_dice_foregrounds = metric[1:].mean().item()\n",
    "\n",
    "    metric_values.append((\n",
    "    epoch + 1,\n",
    "    dice_bg,\n",
    "    dice_cls1,\n",
    "    dice_cls2,\n",
    "    avg_dice_foregrounds\n",
    "    ))\n",
    "\n",
    "    print(f\"Dice(background): {dice_bg:.4f}, Dice(class1): {dice_cls1:.4f}, \"\n",
    "          f\"Dice(class2): {dice_cls2:.4f}, Avg(foregrounds): {avg_dice_foregrounds:.4f}, \"\n",
    "          f\"Avg Dice (including background): {avg_dice_all:.4f}\")\n",
    "\n",
    "    # return foreground dice\n",
    "    return avg_dice_foregrounds\n",
    "\n",
    "\n",
    "# --- training loop with early stopping ---\n",
    "patience = 5        # how many epochs to wait for improvement\n",
    "counter = 0         # no-improve counter\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for step, batch_data in enumerate(train_loader, 1):\n",
    "        images = batch_data[\"image\"].to(device)\n",
    "        labels = batch_data[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        print(f\"  step {step:>3}/{len(train_loader):<3}  loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    print(f\"  Average Train Loss: {avg_train_loss:.4f}\")\n",
    "    epoch_loss_values.append((epoch+1, avg_train_loss))\n",
    "    lr_values.append((epoch+1, optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    # run validation every val_interval epochs\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        val_metric = run_validation(model, val_loader, post_pred, post_label, dice_metric, device, epoch)\n",
    "        print(f\"  Validation Dice: {val_metric:.4f}   (best: {best_metric:.4f} at epoch {best_epoch})\")\n",
    "    \n",
    "        # Update the scheduler based on validation metric\n",
    "        scheduler.step(val_metric)\n",
    "\n",
    "        # early-stopping logic\n",
    "        if val_metric > best_metric:\n",
    "            best_metric = val_metric\n",
    "            best_epoch = epoch + 1\n",
    "            counter = 0\n",
    "\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"best_metric\": best_metric,\n",
    "                },\n",
    "                BEST_MODEL_PATH,\n",
    "            )\n",
    "            print(f\"  --> New best model saved at epoch {best_epoch}!\")\n",
    "\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"  Early stopping: no improvement in {patience} validations.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe4135",
   "metadata": {},
   "source": [
    "Plot training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86533c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Unpack the data from your metric collections.\n",
    "# For the dice metrics (validation), each element in metric_values is:\n",
    "# (epoch, dice_bg, dice_cls1, dice_cls2, avg_dice_foregrounds)\n",
    "epochs_dice, dice_bg, dice_cls1, dice_cls2, avg_dice_fg = zip(*metric_values)\n",
    "\n",
    "# For the training loss (average in-sample error), each element in epoch_loss_values is:\n",
    "# (epoch, avg_train_loss)\n",
    "epochs_loss, train_loss = zip(*epoch_loss_values)\n",
    "\n",
    "# Create a figure with two y-axes.\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot Dice scores on the primary y-axis.\n",
    "ax1.plot(epochs_dice, dice_bg, 'b-o', label='Dice (Background)')\n",
    "ax1.plot(epochs_dice, dice_cls1, color='orange', marker='o', linestyle='-', label='Dice (Class 1)')\n",
    "ax1.plot(epochs_dice, dice_cls2, 'g-o', label='Dice (Class 2)')\n",
    "ax1.plot(epochs_dice, avg_dice_fg, 'k--o', label='Avg Dice (Foregrounds)')\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Dice Score\")\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.tick_params(axis='y')\n",
    "\n",
    "# Create a second y-axis for the training loss.\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs_loss, train_loss, 'r-s', linestyle='--', label='Train Loss (In-sample Error)')\n",
    "ax2.set_ylabel(\"Train Loss\")\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "ax2.set_ylim(0, max(train_loss) * 1.1)  # adjust as needed\n",
    "\n",
    "# Combine legends from both axes.\n",
    "lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc='best')\n",
    "\n",
    "plt.title(\"Validation Dice Scores and Train Loss per Epoch\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
